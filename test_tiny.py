import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Model adÄ±
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print("Downloading and loading TinyLlama (this can take a few minutes on first run)...")

# Tokenizer ve model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16  # FP16 mod, 4bit kapalÄ±
)

print("âœ… Model loaded successfully!\n")

# Test prompt
prompt = "What is the capital of England?"

# Girdi tensoru oluÅŸtur
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Modelden Ã§Ä±ktÄ± al
output = model.generate(**inputs, max_new_tokens=50)

# Sonucu Ã§Ã¶z ve yazdÄ±r
response = tokenizer.decode(output[0], skip_special_tokens=True)
print("ðŸ§  Model Output:\n", response)
